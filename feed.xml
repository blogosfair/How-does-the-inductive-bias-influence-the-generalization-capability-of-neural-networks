<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://blogosfair.github.io/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/feed.xml" rel="self" type="application/atom+xml"/><link href="https://blogosfair.github.io/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-04-28T09:12:52+00:00</updated><id>https://blogosfair.github.io/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/feed.xml</id><title type="html">ICLR Blogposts 2023 (staging)</title><subtitle>Staging website for the 2023 ICLR Blogposts track </subtitle><entry><title type="html">Sample Blog Post</title><link href="https://blogosfair.github.io/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/blog/2022/distill-example/" rel="alternate" type="text/html" title="Sample Blog Post"/><published>2022-12-01T00:00:00+00:00</published><updated>2022-12-01T00:00:00+00:00</updated><id>https://blogosfair.github.io/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/blog/2022/distill-example</id><content type="html" xml:base="https://blogosfair.github.io/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/blog/2022/distill-example/"><![CDATA[<h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example:</p> \[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\] <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <h2 id="images-and-figures">Images and Figures</h2> <p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you might face losing important information in your blog post. To include images in your submission in this way, you must do something like the following:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include figure.html path="assets/img/2022-12-01-distill-example/iclr.png" class="img-fluid" %}
</code></pre></div></div> <p>which results in the following image:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/iclr-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/iclr-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/iclr-1400.webp"/> <img src="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/iclr.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To ensure that there are no namespace conflicts, you must save your asset to your unique directory <code class="language-plaintext highlighter-rouge">/assets/img/2023-05-01-[SUBMISSION NAME]</code> within your submission.</p> <p>Please avoid using the direct markdown method of embedding images; they may not be properly resized. Some more complex ways to load images (note the different styles of the shapes/shadows):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/9-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/9-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/9-1400.webp"/> <img src="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/9.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/7-1400.webp"/> <img src="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/8-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/8-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/8-1400.webp"/> <img src="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/8.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/10-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/10-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/10-1400.webp"/> <img src="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/10.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/11-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/11-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/11-1400.webp"/> <img src="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/11.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/12-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/12-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/12-1400.webp"/> <img src="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/12.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/7-1400.webp"/> <img src="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-distill-example/7.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="interactive-figures">Interactive Figures</h3> <p>Here’s how you could embed interactive figures that have been exported as HTML files. Note that we will be using plotly for this demo, but anything built off of HTML should work (<strong>no extra javascript is allowed!</strong>). All that’s required is for you to export your figure into HTML format, and make sure that the file exists in the <code class="language-plaintext highlighter-rouge">assets/html/[SUBMISSION NAME]/</code> directory in this repository’s root directory. To embed it into any page, simply insert the following code anywhere into your page.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include [FIGURE_NAME].html %} 
</code></pre></div></div> <p>For example, the following code can be used to generate the figure underneath it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="s">'https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv'</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="n">lat</span><span class="o">=</span><span class="s">'Latitude'</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="s">'Longitude'</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="s">'Magnitude'</span><span class="p">,</span> <span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mapbox_style</span><span class="o">=</span><span class="s">"stamen-terrain"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="s">'./assets/html/2022-12-01-distill-example/plotly_demo_1.html'</span><span class="p">)</span>
</code></pre></div></div> <p>And then include it with the following:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">"l-page"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;iframe</span> <span class="na">src=</span><span class="s">"{{ 'assets/html/2022-12-01-distill-example/plotly_demo_1.html' | relative_url }}"</span> <span class="na">frameborder=</span><span class="s">'0'</span> <span class="na">scrolling=</span><span class="s">'no'</span> <span class="na">height=</span><span class="s">"600px"</span> <span class="na">width=</span><span class="s">"100%"</span><span class="nt">&gt;&lt;/iframe&gt;</span>
<span class="nt">&lt;/div&gt;</span>
</code></pre></div></div> <p>Voila!</p> <div class="l-page"> <iframe src="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/html/2022-12-01-distill-example/plotly_demo_1.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <h2 id="citations">Citations</h2> <p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas.</p> <p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it.</p> <p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p> <hr/> <h2 id="footnotes">Footnotes</h2> <p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p> <hr/> <h2 id="code-blocks">Code Blocks</h2> <p>This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag:</p> <p>{% highlight c++ linenos %} <br/> code code code <br/> {% endhighlight %}</p> <p>The keyword <code class="language-plaintext highlighter-rouge">linenos</code> triggers display of line numbers. You can try toggling it on or off yourself below:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="err">\</span><span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure> <hr/> <h2 id="diagrams">Diagrams</h2> <p>This theme supports generating various diagrams from a text description using <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> plugin. Below, we generate a few examples of such diagrams using languages such as <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid</a>, <a href="https://plantuml.com/" target="\_blank">plantuml</a>, <a href="https://vega.github.io/vega-lite/" target="\_blank">vega-lite</a>, etc.</p> <p><strong>Note:</strong> different diagram-generation packages require external dependencies to be installed on your machine. Also, be mindful of that because of diagram generation the fist time you build your Jekyll website after adding new diagrams will be SLOW. For any other details, please refer to <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> README.</p> <p><strong>Note:</strong> This is not supported for local rendering!</p> <p>The diagram below was generated by the following code:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% mermaid %}
sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
{% endmermaid %}
</code></pre></div></div> <div class="jekyll-diagrams diagrams mermaid"> <svg id="mermaid-1682673183625" width="100%" xmlns="http://www.w3.org/2000/svg" height="100%" style="max-width:450px;" viewBox="-50 -10 450 231"><style>#mermaid-1682673183625 .label{font-family:trebuchet ms,verdana,arial;color:#333}#mermaid-1682673183625 .node circle,#mermaid-1682673183625 .node ellipse,#mermaid-1682673183625 .node polygon,#mermaid-1682673183625 .node rect{fill:#ececff;stroke:#9370db;stroke-width:1px}#mermaid-1682673183625 .node.clickable{cursor:pointer}#mermaid-1682673183625 .arrowheadPath{fill:#333}#mermaid-1682673183625 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1682673183625 .edgeLabel{background-color:#e8e8e8}#mermaid-1682673183625 .cluster rect{fill:#ffffde!important;stroke:#aa3!important;stroke-width:1px!important}#mermaid-1682673183625 .cluster text{fill:#333}#mermaid-1682673183625 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:trebuchet ms,verdana,arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1682673183625 .actor{stroke:#ccf;fill:#ececff}#mermaid-1682673183625 text.actor{fill:#000;stroke:none}#mermaid-1682673183625 .actor-line{stroke:grey}#mermaid-1682673183625 .messageLine0{marker-end:"url(#arrowhead)"}#mermaid-1682673183625 .messageLine0,#mermaid-1682673183625 .messageLine1{stroke-width:1.5;stroke-dasharray:"2 2";stroke:#333}#mermaid-1682673183625 #arrowhead{fill:#333}#mermaid-1682673183625 #crosshead path{fill:#333!important;stroke:#333!important}#mermaid-1682673183625 .messageText{fill:#333;stroke:none}#mermaid-1682673183625 .labelBox{stroke:#ccf;fill:#ececff}#mermaid-1682673183625 .labelText,#mermaid-1682673183625 .loopText{fill:#000;stroke:none}#mermaid-1682673183625 .loopLine{stroke-width:2;stroke-dasharray:"2 2";marker-end:"url(#arrowhead)";stroke:#ccf}#mermaid-1682673183625 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1682673183625 .noteText{fill:#000;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:14px}#mermaid-1682673183625 .section{stroke:none;opacity:.2}#mermaid-1682673183625 .section0{fill:rgba(102,102,255,.49)}#mermaid-1682673183625 .section2{fill:#fff400}#mermaid-1682673183625 .section1,#mermaid-1682673183625 .section3{fill:#fff;opacity:.2}#mermaid-1682673183625 .sectionTitle0,#mermaid-1682673183625 .sectionTitle1,#mermaid-1682673183625 .sectionTitle2,#mermaid-1682673183625 .sectionTitle3{fill:#333}#mermaid-1682673183625 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1682673183625 .grid .tick{stroke:#d3d3d3;opacity:.3;shape-rendering:crispEdges}#mermaid-1682673183625 .grid path{stroke-width:0}#mermaid-1682673183625 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1682673183625 .task{stroke-width:2}#mermaid-1682673183625 .taskText{text-anchor:middle;font-size:11px}#mermaid-1682673183625 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1682673183625 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1682673183625 .taskText0,#mermaid-1682673183625 .taskText1,#mermaid-1682673183625 .taskText2,#mermaid-1682673183625 .taskText3{fill:#fff}#mermaid-1682673183625 .task0,#mermaid-1682673183625 .task1,#mermaid-1682673183625 .task2,#mermaid-1682673183625 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1682673183625 .taskTextOutside0,#mermaid-1682673183625 .taskTextOutside1,#mermaid-1682673183625 .taskTextOutside2,#mermaid-1682673183625 .taskTextOutside3{fill:#000}#mermaid-1682673183625 .active0,#mermaid-1682673183625 .active1,#mermaid-1682673183625 .active2,#mermaid-1682673183625 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1682673183625 .activeText0,#mermaid-1682673183625 .activeText1,#mermaid-1682673183625 .activeText2,#mermaid-1682673183625 .activeText3{fill:#000!important}#mermaid-1682673183625 .done0,#mermaid-1682673183625 .done1,#mermaid-1682673183625 .done2,#mermaid-1682673183625 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1682673183625 .doneText0,#mermaid-1682673183625 .doneText1,#mermaid-1682673183625 .doneText2,#mermaid-1682673183625 .doneText3{fill:#000!important}#mermaid-1682673183625 .crit0,#mermaid-1682673183625 .crit1,#mermaid-1682673183625 .crit2,#mermaid-1682673183625 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1682673183625 .activeCrit0,#mermaid-1682673183625 .activeCrit1,#mermaid-1682673183625 .activeCrit2,#mermaid-1682673183625 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1682673183625 .doneCrit0,#mermaid-1682673183625 .doneCrit1,#mermaid-1682673183625 .doneCrit2,#mermaid-1682673183625 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1682673183625 .activeCritText0,#mermaid-1682673183625 .activeCritText1,#mermaid-1682673183625 .activeCritText2,#mermaid-1682673183625 .activeCritText3,#mermaid-1682673183625 .doneCritText0,#mermaid-1682673183625 .doneCritText1,#mermaid-1682673183625 .doneCritText2,#mermaid-1682673183625 .doneCritText3{fill:#000!important}#mermaid-1682673183625 .titleText{text-anchor:middle;font-size:18px;fill:#000}
#mermaid-1682673183625 g.classGroup text{fill:#9370db;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:10px}#mermaid-1682673183625 g.classGroup rect{fill:#ececff;stroke:#9370db}#mermaid-1682673183625 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1682673183625 .classLabel .box{stroke:none;stroke-width:0;fill:#ececff;opacity:.5}#mermaid-1682673183625 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1682673183625 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1682673183625 #compositionEnd,#mermaid-1682673183625 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1682673183625 #aggregationEnd,#mermaid-1682673183625 #aggregationStart{fill:#ececff;stroke:#9370db;stroke-width:1}#mermaid-1682673183625 #dependencyEnd,#mermaid-1682673183625 #dependencyStart,#mermaid-1682673183625 #extensionEnd,#mermaid-1682673183625 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1682673183625 .branch-label,#mermaid-1682673183625 .commit-id,#mermaid-1682673183625 .commit-msg{fill:#d3d3d3;color:#d3d3d3}</style><style>#mermaid-1682673183625{color:#000;font:normal normal 400 normal 16px / normal "Times New Roman"}</style><g></g><g><line id="actor0" x1="75" y1="5" x2="75" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="0" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><line id="actor1" x1="275" y1="5" x2="275" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="200" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g><defs><marker id="arrowhead" refX="5" refY="2" markerWidth="6" markerHeight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><defs><marker id="crosshead" markerWidth="15" markerHeight="8" orient="auto" refX="16" refY="4"><path fill="black" stroke="#000000" stroke-width="1px" d="M 9,2 V 6 L16,4 Z" style="stroke-dasharray: 0, 0;"></path><path fill="none" stroke="#000000" stroke-width="1px" d="M 0,1 L 6,7 M 6,1 L 0,7" style="stroke-dasharray: 0, 0;"></path></marker></defs><g><text x="175" y="93" class="messageText" style="text-anchor: middle;">Hello John, how are you?</text><line x1="275" y1="100" x2="75" y2="100" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="175" y="128" class="messageText" style="text-anchor: middle;">Great!</text><line x1="75" y1="135" x2="275" y2="135" class="messageLine1" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="stroke-dasharray: 3, 3; fill: none;"></line></g><g><rect x="0" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><rect x="200" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g></svg> </div> <hr/> <h2 id="tweets">Tweets</h2> <p>An example of displaying a tweet:</p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>An example of pulling from a timeline:</p> <div class="jekyll-twitter-plugin"><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a></p> <hr/> <h2 id="blockquotes">Blockquotes</h2> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin </blockquote> <hr/> <h2 id="layouts">Layouts</h2> <p>The main text column is referred to as the body. It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p> <div class="fake-img l-body"> <p>.l-body</p> </div> <p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p> <div class="fake-img l-page"> <p>.l-page</p> </div> <p>All of these have an outset variant if you want to poke out from the body text a little bit. For instance:</p> <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> <p>Occasionally you’ll want to use the full browser width. For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>. You can also inset the element a little from the edge of the browser by using the inset variant.</p> <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> <p>The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code> sized text except on mobile screen sizes.</p> <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <hr/> <h2 id="other-typography">Other Typography?</h2> <p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p> <p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p> <p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p> <p>Strikethrough uses two tildes. <del>Scratch this.</del></p> <ol> <li>First ordered list item</li> <li>Another item ⋅⋅* Unordered sub-list.</li> <li>Actual numbers don’t matter, just that it’s a number ⋅⋅1. Ordered sub-list</li> <li>And another item.</li> </ol> <p>⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p> <p>⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅ ⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅ ⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)</p> <ul> <li>Unordered list can use asterisks</li> <li>Or minuses</li> <li>Or pluses</li> </ul> <p><a href="https://www.google.com">I’m an inline-style link</a></p> <p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p> <p><a href="https://www.mozilla.org">I’m a reference-style link</a></p> <p><a href="../blob/master/LICENSE">I’m a relative reference to a repository file</a></p> <p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p> <p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p> <p>URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes example.com (but not on Github, for example).</p> <p>Some text to show that the reference links can follow later.</p> <p>Here’s our logo (hover to see the title text):</p> <p>Inline-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1"/></p> <p>Reference-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2"/></p> <p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="s">"Python syntax highlighting"</span>
<span class="k">print</span> <span class="n">s</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting. 
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div> <p>Colons can be used to align columns.</p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p>There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don’t need to make the raw Markdown line up prettily. You can also use inline Markdown.</p> <table> <thead> <tr> <th>Markdown</th> <th>Less</th> <th>Pretty</th> </tr> </thead> <tbody> <tr> <td><em>Still</em></td> <td><code class="language-plaintext highlighter-rouge">renders</code></td> <td><strong>nicely</strong></td> </tr> <tr> <td>1</td> <td>2</td> <td>3</td> </tr> </tbody> </table> <blockquote> <p>Blockquotes are very handy in email to emulate reply text. This line is part of the same quote.</p> </blockquote> <p>Quote break.</p> <blockquote> <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p> </blockquote> <p>Here’s a line for us to start with.</p> <p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p> <p>This line is also a separate paragraph, but… This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. This is an example of a distill-style blog post and the main elements it supports.]]></summary></entry><entry><title type="html">How does the inductive bias influence the generalization capability of neural networks?</title><link href="https://blogosfair.github.io/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/blog/2022/how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/" rel="alternate" type="text/html" title="How does the inductive bias influence the generalization capability of neural networks?"/><published>2022-12-01T00:00:00+00:00</published><updated>2022-12-01T00:00:00+00:00</updated><id>https://blogosfair.github.io/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/blog/2022/how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks</id><content type="html" xml:base="https://blogosfair.github.io/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/blog/2022/how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/"><![CDATA[<p>Deep neural networks are a commonly used machine learning technique that has proven to be effective for many different use cases. However, their ability to generalize from training data is not well understood. In this blog post, we will explore the paper “Identity Crisis: Memorization and Generalization under Extreme Overparameterization” by Zhang et al. [2020] <d-cite key="DBLP:conf/iclr/ZhangBHMS20"></d-cite>, which aims to shed light on the question of why neural networks are able to generalize, and how inductive biases influence their generalization capabilities.</p> <h2 id="overfitting-puzzle">Overfitting Puzzle</h2> <p>One open question in the field of machine learning is the <strong>overfitting puzzle</strong>, which describes the paradox that neural networks are often used in an overparameterized state (i.e., with more parameters than training examples), yet they are still able to generalize well to new, unseen data. This contradicts <strong>classical learning theory</strong>, which states that a model with too many parameters will simply memorize the training data and perform poorly on new data. This is based on the <a href="https://machinelearningcompass.com/model_optimization/bias_and_variance/"><strong>bias-variance tradeoff</strong></a> which is commonly illustrated in this way <d-cite key="fortmann2012understanding"></d-cite>:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/bias_variance_tradeoff-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/bias_variance_tradeoff-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/bias_variance_tradeoff-1400.webp"/> <img src="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/bias_variance_tradeoff.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The tradeoff consists of finding the optimal model complexity between two extremes: If there are too few parameters, the model may have high bias and underfit the data, resulting in poor performance on both the training and test data. On the other hand, if there are too many parameters, the model may have high variance and overfit the training data, resulting in a good performance on the training data but a poor performance on the test data.</p> <p>Therefore, it is important to carefully balance the number of parameters and the amount of data available to achieve the best possible generalization performance for a given learning task.</p> <p>Neural networks, particularly deep networks, are typically used in the overparameterized regime, where the number of parameters exceeds the number of training examples. In these cases, common generalization bounds do not apply <d-cite key="DBLP:journals/corr/abs-1801-00173"></d-cite>. According to classical learning theory, the generalization behavior of a learning system should depend on the number of training examples (n), and the complexity of the model should be balanced with its fit to the data <d-cite key="DBLP:journals/corr/abs-1801-00173"></d-cite>. Otherwise, the algorithm would overfit. However, neural networks have shown that this is not always the case, as they can perform well even in cases of extreme overparameterization (e.g., a 5-layer CNN with 80 million parameters <d-cite key="DBLP:conf/iclr/ZhangBHMS20"></d-cite>). This is a very interesting finding as it shows that the classical learning theory may not hold true for neural networks.</p> <p>To better understand this phenomenon, Zhang et al. [2020] <d-cite key="DBLP:conf/iclr/ZhangBHMS20"></d-cite> examined the role of <strong>inductive bias</strong> in neural networks and its influence on the generalization capability of these networks. Inductive bias, or learning bias, refers to the assumptions a network makes about the nature of the target function and is determined by the network’s architecture. Zhang et al. [2020] <d-cite key="DBLP:conf/iclr/ZhangBHMS20"></d-cite> conducted experiments with different types of fully connected networks (FCN) and convolutional neural networks (CNN) to investigate which biases are effective for these network architectures.</p> <h2 id="experiments">Experiments</h2> <p>In the paper “Identity Crisis: Memorization and Generalization under Extreme Overparameterization” by Zhang et al. [2020] <d-cite key="DBLP:conf/iclr/ZhangBHMS20"></d-cite>, the authors use <strong>empirical studies</strong> to better understand the <em>overfitting puzzle</em> and how inductive bias affects the behavior of overparameterized neural networks. The authors specifically aim to investigate the role of inductive bias under <strong>different architectural choices</strong> by comparing fully connected and convolutional neural networks.</p> <p>The task used in the study is to learn an identity map through a single data point, which is an artificial setup that demonstrates the most extreme case of overparameterization. The goal of the study is to determine whether a network tends towards memorization (learning a constant function) or generalization (learning the identity function).</p> <p>To enable the <strong>identity task</strong> <d-cite key="DBLP:conf/eccv/HeZRS16"></d-cite> for linear models, the authors ensure that hidden dimensions are not smaller than the input and set the weights to the identity matrix in every layer. For convolutional layers, only the center of the kernel is used, and all other values are set to zero, simulating a 1 x 1 convolution which acts as a local identity function. For deeper models that use the <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a> activation function, it is necessary to encode and recover negative values, as they are discarded by the ReLU function. This can be achieved by using hidden dimensions that are twice the size of the input and storing negative and positive values separately.</p> <p>All networks are trained using standard gradient descent to minimize the mean squared error.</p> <p>The study uses the <strong><a href="https://paperswithcode.com/dataset/mnist">MNIST dataset</a></strong> and tests the networks on various types of data, including a linear combination of two digits, random digits from the MNIST test set, random images from the Fashion MNIST dataset, and algorithmically generated image patterns.</p> <p>So let us look at some of the results:</p> <div class="l-page"> <iframe src="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/html/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_3.html" frameborder="0" scrolling="no" width="100%" height="450px"></iframe> </div> <p>The first column of the figure above shows the single data point that was used to train the network on, and all following columns show the test data with its specific results. The rows represent the different implementations of the respective networks (FCN, CNN).</p> <h3 id="fully-connected-networks-fcn">Fully connected networks (FCN)</h3> <p>For fully connected networks, the outputs differ depending on the depth of the network and the type of testing data. Shallower networks seem to incorporate random white noise into the output, while deeper networks tend to learn the constant function. The similarity of the test data to the training example also affects the behavior of the model. When the test data is from the MNIST digit sets, all network architectures perform quite well. However, for test data that is more dissimilar to the training data, the output tends to include more random white noise. The authors prove this finding with a <em>theorem</em> for 1-layer FCNs. The formula shows the prediction results for a test data point $x$:</p> \[f(x) = \Pi_{\parallel}(x) + R \Pi_{\perp}(x)\] <p>The test data point is decomposed into components that are parallel $\Pi_{\parallel}$ and perpendicular $\Pi_{\perp}$ to the training example. $R$ is a random matrix, independent of the training data. If the test data is highly correlated to the training data, the prediction resembles the training output. If the test data is dissimilar to the training data, $\Pi_{\perp}(x)$ dominates $\Pi_{\parallel}(x)$, the output is randomly projected by $R$ and persists of white noise.</p> <p>This behavior can be confirmed by visualizing the results of the 1-layer FCN:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_1layer-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_1layer-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_1layer-1400.webp"/> <img src="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_1layer.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The inductive bias does not lead to either good generalization or memorization. Instead, the predictions become more random as the test data becomes less similar to the training data.</p> <p>Deeper networks tend to learn the constant function, resulting in a strong inductive bias towards the training output regardless of the specific input. This behavior is similar to that of a deep ReLU network, as shown in the figure comparing deep FCN and deep ReLU networks.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_compareFCNReLU-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_compareFCNReLU-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_compareFCNReLU-1400.webp"/> <img src="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/img/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/Figure2_compareFCNReLU.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Zhang et al. [2020] <d-cite key="DBLP:conf/iclr/ZhangBHMS20"></d-cite> conclude that more complex network architectures are more prone to memorization. This finding aligns with statistical learning theory, as a more complex architecture has more parameters and, therefore, more overparameterization.</p> <h3 id="convolutional-neural-networks-cnn">Convolutional neural networks (CNN)</h3> <p>For convolutional neural networks, the inductive bias was analyzed using the ReLU activation function and testing networks with different depths. The hidden layers of the CNN consist of 5 × 5 convolution filters organized into 128 channels. The networks have two constraints to match the structure of the identity target function.</p> <p>If you choose the button ‘CNN’ in the first figure, it shows the resulting visualizations. It can be seen that shallow networks are able to learn the identity function, while intermediate-depth networks function as edge detectors, and deep networks learn the constant function. Whether the model learns the identity or the constant function, both outcomes reflect inductive biases since no specific structure was given by the task.</p> <p>A better understanding of the evolution of the output can be obtained by examining the status of the prediction in the hidden layers of the CNN. Since CNNs, unlike FCNs, preserve the spatial relations between neurons in the intermediate layers, these layers can be visualized. The figure below shows the results for a randomly initialized 20-layer CNN compared to different depths of trained CNNs.”</p> <div class="l-page"> <iframe src="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/html/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/CNNs_intermedLayers.html" frameborder="0" scrolling="no" width="100%" height="450px"></iframe> </div> <p>Random convolution gradually smooths out the input data, and after around eight layers, the shapes are lost. When the networks are trained, the results differ. The 7-layer CNN performs well and ends up with an identity function of the input images, while the results of the 14-layer CNN are more blurry. For the 20-layer trained CNN, it initially behaves similarly to the randomly initialized CNN by wiping out the input data, but it preserves the shapes for a longer period. In the last three layers, it renders the constant function of the training data and outputs 7 for any input.</p> <p>These results align with the findings of Radhakrishnan et al. [2018] <d-cite key="radhakrishnan2019memorization"></d-cite> in ‘Memorization in overparametrized autoencoders’, which used a similar empirical framework on fully-connected autoencoders. They found that deep neural networks learn locally contractive maps around the training examples, leading to learning the constant function.</p> <p>As for FCNs, the experiments show that the similarity of the test data to the training data point increases task success. Zhang et al. [2020] <d-cite key="DBLP:conf/iclr/ZhangBHMS20"></d-cite> conducted further experiments with different <strong>feature channel numbers and dimensions</strong>. They found that increasing the hidden dimensions/adding channels is much less prone to overfitting than adding depth. This should be considered when designing new models: if the goal is to increase the number of parameters of an existing model (perhaps to improve optimization dynamics or prepare for more training data), it is better to try increasing the hidden dimension before tuning the depth, unless the nature of the data changes.</p> <p>Another factor that influences inductive bias is **model initialization++. For networks with few channels, the difference between random initialization and the converged network is extreme <d-cite key="DBLP:conf/iclr/FrankleC19"></d-cite>. This can be explained as follows: in the regime of random initialization with only a few channels, the initialization does not have enough flexibility to compensate for incorrect choices. As a result, the networks are more likely to converge to non-optimal extrema. Having more channels helps to smooth out this problem, as more parameters can compensate for ‘unlucky’ cases.</p> <h2 id="general-findings">General findings</h2> <p>The first figure in this post shows that CNNs have better generalization capability than FCNs. However, it is important to note that the experiments primarily aim to compare different neural networks <strong>within their architecture type</strong>, so a comparison between FCNs and CNNs cannot be considered fair. CNNs have natural advantages due to sparser networks and structural biases, such as local receptive fields and parameter sharing, that are consistent with the identity task. Additionally, CNNs have more parameters, as seen in the underlying figure: a 6-layer FCN contains 3.6M parameters, while a 5-layer CNN (with 5x5 filters of 1024 channels) has 78M parameters. These differences should be taken into account when evaluating the results of the experiments.</p> <div class="l-page" style="width: 704px; margin: auto;"> <iframe src="/How-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/assets/html/2022-12-01-how-does-the-inductive-bias-influence-the-generalization-capability-of-neural-networks/plot.html" frameborder="0" scrolling="no" width="100%" height="480px"></iframe> </div> <p>To conclude, CNNs generalize better than FCNs, even though they have more parameters. This is consistent with the observed phenomenon that neural networks do not follow the statistical learning theory.</p> <p>The experiments described above lead to the following main findings of the paper:</p> <ul> <li>The number of parameters does not strongly correlate with generalization performance, but the structural bias of the model does.</li> </ul> <p>For example, when equally overparameterized,</p> <ul> <li>training a very deep model is prone to memorization, while</li> <li>adding more feature channels/dimensions is much less likely to cause overfitting.</li> </ul> <h2 id="conclusion">Conclusion</h2> <p>After reading this blog post, we hope that the concept of the overfitting puzzle is understood and it is revealed how the generalization capability of neural networks contrasts with classical learning theory. We also made the significance of the study conducted by Zhang et al. [2020] <d-cite key="DBLP:conf/iclr/ZhangBHMS20"></d-cite> clear, as they provide more insights into the inductive bias. The artificial setup used in the study is a smart way to approach this topic and allows for an intuitive interpretation of the results. The authors found that CNNs tend to <em>generalize</em> by actually learning the concept of identity, while FCNs are prone to memorization. Within these networks, it can be said that the simpler the network architecture is, the better the task results. Another observation is that deep CNNs exhibit extreme memorization. It would have been interesting to analyze the inductive bias for other types of data (e.g., sequence data like speech) and compare whether the stated theorems also hold in those cases.</p> <p>In summary, Zhang et al. [2020] <d-cite key="DBLP:conf/iclr/ZhangBHMS20"></d-cite> conducted interesting studies that have helped the machine learning community to gain a deeper understanding of inductive bias. Their results provide concrete guidance for practitioners that can help design models for new tasks.</p>]]></content><author><name>Charlotte Barth</name></author><summary type="html"><![CDATA[["The blog post discusses how memorization and generalization are affected by extreme overparameterization. Therefore", "it explains the *overfitting puzzle* in machine learning and how the *inductive bias* can help to understand the generalization capability of neural networks."]]]></summary></entry></feed>